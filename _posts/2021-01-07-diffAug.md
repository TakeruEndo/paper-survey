---
layout: post
title: Differentiable Augmentation for Data-Efficient GAN Training
summary: 微分可能なDAによってGANの学習を安定化させた
featured-img: mnist
categories: CV GAN DataAugmentation
---

## 1. どんなもの？

<img width="600" alt="スクリーンショット 2021-01-07 12 48 19" src="https://user-images.githubusercontent.com/40351074/103848831-a2dc9f80-50e6-11eb-9bd4-75b33fcf9c32.png">

少量の画像で学習を安定させるためのDataAugmentationを提案。
DiffAugmentは、生成されたサンプルに対して微分可能なオーグメンテーションを採用することができ、学習を効果的に安定化させ、収束性を向上させることができる。
CIFAR-10やCIFAR-100で従来の20%のデータでトップ性能に匹敵する結果を得られた。事前学習を行わずにたった100枚の画像を用いて高忠実度の画像を生成することができる。

**https://github.com/mit-han-lab/data-efficient-gans**


## 2. 先行研究と比べてどこがすごいの？
従来のGANでは、学習画像の枚数を制限すると、trainLossが早期に収束し、accuracyは0.3程度に下がってしまう。

<img width="500" alt="スクリーンショット 2021-01-07 11 46 42" src="https://user-images.githubusercontent.com/40351074/103844851-2e056780-50de-11eb-994d-117d7a73d775.png"></img>

データ拡張には、cropping, flipping, scaling, color jittering、region masking (Cutout)など、様々な手法があるがGANには適していない。それは、生成器がデータ拡張により不自然な画像の分布を学習してしまうからだ。また、識別器と生成器の両方にDAを施した場合、両者の微妙なバランスが崩れてしまってうまくいかない。

本手法では、実画像と偽画像の両方に同じ微分可能な拡張を適用している。DiffAugmentを用いることで、グラディエントをオーグメンテーションを介してGeneratorに伝搬させることができ、ターゲット分布を操作することなく判別器を正規化し、学習ダイナミクスのバランスを維持することができる。

### 学習データを削減させることを試みる研究
1. Self-supervised gans viaauxiliary rotation loss. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
2. High-Fidelity Image Generation With Fewer Labels. In International Conference on Machine Learning (ICML), 2019.

### GANにおけるこれまでの正則化手法
入力の局所的な領域内での識別器の出力の急激な変化に対して罰則を設けている。
1. the instance noise
2. Jensen-Shannon regularization
3. gradient penalties (wasserstein gans)
4. adversarial defense regularization (Don’t let your discriminator be fooled)
5. consistency regularization (Consistency regularization for generative adversarial networks)

### データ拡張
1. color jittering
2. region masking
3. flipping, rotation, cropping
4. data mixing

そもそも、<span style="color: red;">**データ拡張をGANに直接適用してもベースラインは改善されないことが示されている。**</span>

では、なぜGANにデータ拡張を適用しても、分類器の拡張ほど効果的ではないのか？？

これまでほとんどのGANでは、 生成器が学習する分布を適切に保つためhorizontal flipsのみしかDAで用いてこなかった。以下の表は、DAによって性能が低下した結果を表す。  
<img width="600" alt="スクリーンショット 2021-01-07 12 18 35" src="https://user-images.githubusercontent.com/40351074/103846912-850d3b80-50e2-11eb-8c42-eede9cd857f2.png">

また、生成器からの出力とリアル画像の両方にDAを用いた場合、上記の表で見られるように、T(G(z))とT(x)は見分けられるが、G(z)を識別できなくなってしまう。

## 3. 技術や手法の"キモ"はどこにある？
<img width="650" alt="スクリーンショット 2021-01-07 12 36 39" src="https://user-images.githubusercontent.com/40351074/103848120-02d24680-50e5-11eb-966b-9099dc4055dd.png">
<img width="500" alt="スクリーンショット 2021-01-07 12 37 09" src="https://user-images.githubusercontent.com/40351074/103848148-12ea2600-50e5-11eb-9cd6-ea0828da4938.png">

微分可能なDA(DiffAugment)を用いて生成器を更新させることで、生成器がデータ拡張の影響を考慮できるようにした。

### 用いたデータ拡張
- 画像サイズの[-1/8, 1/8]以内、ゼロでパディング
- カットアウト（画像サイズの半分のランダムな正方形でマスキング）
- カラー変換（[-0.5, 0.5]以内のランダムな明るさ、[0.5, 1.5]以内のコントラスト、[0, 2]以内の彩度)

<img width="600" alt="スクリーンショット 2021-01-07 12 44 11" src="https://user-images.githubusercontent.com/40351074/103848589-0f0ad380-50e6-11eb-89a2-842b9843325e.png">

## 4. どうやって有効だと検証した？
CIFAR-10, CIFAR-100などを用いて、評価指標はFID

<img width="600" alt="スクリーンショット 2021-01-07 12 56 43" src="https://user-images.githubusercontent.com/40351074/103849386-dcfa7100-50e7-11eb-9970-57361f5558c4.png">

## 5. 議論はあるか？
DiifAugを用いた場合、モデルが大きくてもstyleGANのR1正則化を強くかけてもFIDは高い水準となっている。

<img width="700" alt="スクリーンショット 2021-01-07 12 59 41" src="https://user-images.githubusercontent.com/40351074/103849576-6d38b600-50e8-11eb-8e5c-b9a83d6cc196.png">


## 6. 次に読むべき論文はあるか？
- Andrew Brock, Jeff Donahue, and Karen Simonyan. Large Scale GAN Training for High Fidelity Natural Image Synthesis. In International Conference on Learning Representations (ICLR), 2018.
- Self-supervised gans viaauxiliary rotation loss. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
- Tero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative Adversar- ial Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
- Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and
Improving the Image Quality of StyleGAN. arXiv, 2019.


### 論文情報・リンク

* [Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, Song Han，"Differentiable Augmentation for Data-Efficient GAN Training，" arxiv名，2020](https://arxiv.org/abs/2006.10738)
